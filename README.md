
# Churn Prediction Pipeline

A scalable, cloud-ready Machine Learning pipeline for predicting customer churn â€” built with **Docker**, **Terraform**, **GitHub Actions**, and **AWS ECS**.


> ğŸ“¦ Provisioned with Terraform Â· ğŸš€ Deployed with ECS Fargate Â· ğŸ” Artifacts stored in S3

## Touch the working solution live!
Click [here](https://github.com/sami0i/churn-prediction/actions/runs/15084115059) to see the project demo.
![image](https://github.com/user-attachments/assets/a9685aba-2428-464f-a8a5-6cddafa4c2db)


## Project Structure

```bash
churn-prediction/
â”œâ”€â”€ src/                     # Core pipeline: ingestion, preprocessing, training, prediction
â”‚   â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ config/
|   â”œâ”€â”€ constants/
|   â”œâ”€â”€ entity/
â”‚   â”œâ”€â”€ pipeline/
â”‚   â””â”€â”€ utils/
â”œâ”€â”€ config/                  # YAML configs for pipeline stages
â”œâ”€â”€ data/                    # Sample input dataset (CSV)
â”œâ”€â”€ artifacts/              # Saved model, encoder, scaler, etc.
â”œâ”€â”€ infrastructure/          # Terraform + Ansible for infra as code
â”‚   â”œâ”€â”€ terraform/
â”‚   â””â”€â”€ ansible/
â”œâ”€â”€ .github/workflows/       # GitHub Actions CI/CD
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ params.yaml
â””â”€â”€ main.py
```

## Features

- âœ… **Modular ML Pipeline** â€” Data ingestion, preprocessing, model training, and prediction stages.
- âœ… **Config-Driven** â€” Easily change parameters and file paths via `config.yaml` and `params.yaml`.
- âœ… **CI/CD Pipeline** â€” Auto-build, push, and deploy via GitHub Actions + AWS ECS.
- âœ… **Infrastructure as Code** â€” Provision cloud resources with Terraform + Ansible.
- âœ… **S3 Integration** â€” Artifacts (model, encoder, scaler) stored and loaded from Amazon S3.

---


## âœ… Satisfaction Assessment

This section evaluates how each requirement of the problem statement is satisfied in the churn prediction project.

---

## 1. Scalable Containerized Solution Supporting Multiple Markets

- **Design Choice:** Market is treated as a categorical feature and **one-hot encoded** in the preprocessing pipeline.
- **New Market Ready:** Thanks to `handle_unknown="ignore"`, the pipeline won't break if a previously unseen market appears at inference time.
- **Why Not One Model Per Market?** That approach would fail on new markets and require retraining per region. The unified model design supports generalization and maintenance efficiency.
- **Data Scalability:** Dataset is read from **Amazon S3**, allowing ingestion of large datasets beyond local memory constraints.

---

## 2. Infrastructure via Terraform

- **ğŸ“ Folder:** `infrastructure/terraform/`
- **Resources Created:** S3 buckets, IAM roles and policies, ECS cluster, ECR repo, task definitions, and CloudWatch logs.
- **State Management:** Backend state is stored in S3 and locked with DynamoDB for consistency.
- **Documentation:** High-level explanation is available in `PREDICT.md`.
- **Usage:** Can be run independently or through GitHub Actions.

```bash
cd infrastructure/terraform
terraform init
terraform apply
```

---

## 3. Artifact Upload with Ansible

- **ğŸ“ Folder:** `infrastructure/ansible/upload_artifacts_to_s3.yml`
- **Function:** Uploads trained model artifacts (`.joblib` files) to the appropriate folder in S3.
- **Command:**

```bash
ansible-playbook infrastructure/ansible/upload_artifacts_to_s3.yml
```

- **Purpose:** Demonstrates separation of provisioning (Terraform) and post-processing tasks (Ansible), a real-world best practice.

---

## 4. CI/CD Pipeline Triggered on Commit

- **ğŸ“ Config:** `.github/workflows/`  
- **Platform:** GitHub Actions  
- **Triggers:** Pipeline starts on any commit to any branch.
- **Branch Awareness:** Uses `${GITHUB_REF_NAME}` to print current branch name.
- **Stages:**
  - Terraform infra provisioning
  - Docker image build + push to ECR
  - ECS task execution for training/inference

---

## ğŸ“„ 5. Documentation and Usage Guidelines

- **README.md:** Clearly explains the full project:
  - Setup and installation
  - CLI usage for training and prediction
  - Docker execution and CI/CD process
  - Infrastructure steps with Terraform and Ansible
- **Structure:** Folder-level and file-level structure is documented with visuals and examples.
- **Ease of Use:** One command to train, one to predict.

```bash
python main.py --mode=train
python main.py --mode=predict --input=data/data.csv
```

---

âœ… This project meets all functional and engineering requirements using scalable, modular, and production-grade tooling.




## Setup Instructions

### 1. Clone the Repo

```bash
git clone https://github.com/YOUR_USERNAME/churn-prediction.git
cd churn-prediction
```

### 2. Install Requirements

```bash
pip install -r requirements.txt
```

### 3. Train the Model

```bash
python main.py --mode=train
```

### 4. Run Prediction

```bash
python main.py --mode=predict --input=data/data.csv
```

Predictions will be saved to `results/results.npy`.

## Run with Docker

```bash
docker build -t churn-prediction .
docker run --rm   -e BUCKET_NAME=your-s3-bucket-name   -v $(pwd)/data:/app/data   -v $(pwd)/results:/app/results   churn-prediction   python main.py --mode=predict --input=data/data.csv
```

## Infrastructure

Provisioned using Terraform (S3, IAM, ECS, ECR) and Ansible for local uploads.

```bash
cd infrastructure/terraform
terraform init && terraform apply
```

```bash
ansible-playbook infrastructure/ansible/upload_artifacts_to_s3.yml
```

## CI/CD with GitHub Actions

On each push:
- Prints branch name
- Terraform provisions infra (conditionally)
- Docker builds + pushes to ECR
- ECS runs a container for training/prediction
- Optionally logs evaluation score / metrics


##  Artifacts in S3

- `artifacts/encoder.joblib`
- `artifacts/scaler.joblib`
- `artifacts/models/base_model.joblib`

These are used by the prediction pipeline and uploaded automatically.




## Future Work & Real-World Enhancements

This section outlines practical extensions and improvements to evolve this assessment-level solution into a production-ready machine learning system.

---

### ğŸ§ª 1. Model Evaluation & Thresholding

- Currently, model evaluation is not part of the pipeline.
- Future work should include evaluation metrics such as accuracy, F1-score, precision, recall, and ROC-AUC.
- Visuals like confusion matrices and classification reports should also be included.
- Conditional deployment can be triggered based on threshold metrics.

---

### ğŸ“¡ 2. Experiment Tracking

- Integrate **MLflow**, **Weights & Biases**, or similar tools.
- Automatically log:
  - Hyperparameters
  - Metrics
  - Model versions
  - Code versions
- Enables reproducibility and better model governance.

---

### ğŸ›ï¸ 3. Advanced Training Features

- Use callbacks like:
  - `early_stopping_rounds`
  - Evaluation sets during training
- Improves performance and avoids overfitting.

---

### ğŸ” 4. Monitoring in Production

- Detect:
  - Data drift / concept drift
  - Service latency
  - System errors

---

### â™»ï¸ 5. Automated Retraining

- Set up retraining triggers based on:
  - Scheduled time (e.g., weekly)
  - Performance degradation on live data
  - Drift detection alerts

---

### ğŸŒ 6. Real-Time Inference

- Extend batch pipeline to real-time APIs using:
  - **FastAPI** or **Flask**
  - **AWS Lambda** + **API Gateway**
- Allow low-latency, scalable serving of predictions.

---

### ğŸ” 7. Security and IAM Best Practices

- Add bucket encryption and tighter IAM role definitions.
- Enable CloudTrail for auditing access to resources.

---

### ğŸ“ 8. CI/CD Optimization

- â›”ï¸ Currently, CI/CD runs even on doc-only commits.
- âœ… In production, you would **skip actions** when only `.md`, `.txt`, or `.yaml` files are modified:
  ```yaml
  if: "!contains(github.event.head_commit.message, '[skip ci]')"
  ```
- For this project, we allowed doc-only triggers for **demo testing** purposes.

---
